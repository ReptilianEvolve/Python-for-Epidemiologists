{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Management\n",
    "i.e. introduction to `numpy` and `pandas`\n",
    "\n",
    "Now that we have a handle on the basics of Python 3.x syntax, let's start doing some data manipulation. The two most important packages we will be using are NumPy and Pandas. NumPy contains loads of useful functions and improved storage objects, like matrices (think improved lists) and functions that produce various calculations. Pandas is built on top of NumPy and allows traditional data storage, display, and tools. Like all of our code, we begin with importing the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may already know, the ``import ... as ...`` functionality allows us to import a package and refer to it as a shorter name. It is common practice to abbreviate `numpy` as `np` and `pandas` as `pd`. In your code, *never* set variables to have these names (unless you really want to give yourself a headache)\n",
    "\n",
    "## Missing Data\n",
    "Unlike most python data science intros, I am going to start with missing data. Missing data is a common issue in an epidemiologist's data sets. Simply ignoring missing data and how `numpy` and `pandas` handle missing data is only going to cause later trouble for us. So, how does Python handle missing data?\n",
    "\n",
    "Since Python was original meant as a computer programming language, missing data is a little awkward. The `math` library allows for missing data with `nan`. More commonly you will use `np.nan`. `nan` has some properties that may be surprising. `nan` will never evaluate to be equal to `nan`. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird... but why does this matter? For any data manipulation operations with missing data in a column, you will need to keep this in mind. Expecting something to be equal to `nan` won't evaluate to be true. Instead, you will need to use special `numpy` or `pandas` functions like `isnan()`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(np.nan))\n",
    "print(pd.isnull(np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So remember, **nan is not equal to nan**\n",
    "\n",
    "The next item to highlight is a `pandas` behavior. When you have a column with `int` data that has missing values, it will be stored as a `float` type. This does not lead to any major issues, but if you *need* a column to be of `int` type, know that there can be no missing data. No matter how hard you try to change the type to `int`, it will remain `float`. If you absolutely need that column to be `int`, then you will need to remove the `nan`'s (or give them some dummy integer value). You will likely never need a column to absolutely have to be `int` though\n",
    "\n",
    "## Loading Data\n",
    "This next section details reading in data from outside file formats.\n",
    "\n",
    "### CSV \n",
    "Let's review how to read in CSV files. CSV stands for \"comma separated values\". I have put a sample CSV file in this tutorial folder labeled `sample_csv.csv`. To load this data file using pandas, we use the `read_csv()` function. Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      "A         100000 non-null int64\n",
      "L         100000 non-null float64\n",
      "B_true    100000 non-null int64\n",
      "C_true    100000 non-null int64\n",
      "M2        100000 non-null int64\n",
      "M3        100000 non-null int64\n",
      "B         87168 non-null float64\n",
      "C         27432 non-null float64\n",
      "id        100000 non-null int64\n",
      "dtypes: float64(3), int64(6)\n",
      "memory usage: 6.9 MB\n"
     ]
    }
   ],
   "source": [
    "# If CSV is in the same folder as your Python program\n",
    "df = pd.read_csv('sample_csv.csv')\n",
    "df.info()\n",
    "\n",
    "# Explicit path to file\n",
    "# df = pd.read_csv('C:/file/path/to/sample_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`read_csv()` has lots of options to help with reading in your data. You can set the column names, skip certain rows/columns, and much more. I recommend reviewing the `pandas` documentation for further details (*hint* try searching for \"pandas read_csv\" in your search engine of choice)\n",
    "\n",
    "### DAT\n",
    "`.dat` files are a more universal format you may run in to. They have a wider option of what are referred to as delimiters. Delimiters mark where a particular cell/column ends. Another common delimiter is tabs (known as tab-delimited files). I have another file of example data labelled as `sample_tab.dat`. Let's read that in that data.\n",
    "\n",
    "We will use a few different options. Our input data does not have an index, so we set `index_col=False`. Additionally, our data set does not have column labels, so we set `header=None` to prevent our first line of data from becoming the column labels. We can manually create those by passing in the `names` option. We set `names` to be equal to our list of column labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27382 entries, 0 to 27381\n",
      "Data columns (total 12 columns):\n",
      "id       27382 non-null int64\n",
      "enter    27382 non-null int64\n",
      "out      27382 non-null float64\n",
      "male     27382 non-null int64\n",
      "age0     27382 non-null int64\n",
      "cd40     27382 non-null int64\n",
      "dvl0     27382 non-null int64\n",
      "cd4      27382 non-null int64\n",
      "dvl      27382 non-null int64\n",
      "art      27382 non-null int64\n",
      "drop     27382 non-null int64\n",
      "dead     27382 non-null int64\n",
      "dtypes: float64(1), int64(11)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "cols = ['id', 'enter', 'out', 'male', 'age0', 'cd40', 'dvl0', 'cd4', 'dvl', 'art', 'drop', 'dead']\n",
    "\n",
    "# tab-delimited DAT file\n",
    "df = pd.read_csv('sample_dat.dat', delim_whitespace=True, header=None, names=cols, index_col=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `read_csv()` is much more than only CSV files. \n",
    "\n",
    "### SAS7BDAT\n",
    "Since I sometimes have to load in SAS data files, let's go through an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 547 entries, 0 to 546\n",
      "Data columns (total 8 columns):\n",
      "id      547 non-null float64\n",
      "male    547 non-null float64\n",
      "age0    547 non-null float64\n",
      "cd40    547 non-null float64\n",
      "dvl0    547 non-null float64\n",
      "art     547 non-null float64\n",
      "dead    517 non-null float64\n",
      "t       547 non-null float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 34.3 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sas('sample_sas.sas7bdat')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SAS file for the remainder of the data management tutorial\n",
    "\n",
    "### Other File Formats\n",
    "Python supports a wide variety of formats. I have only reviewed popular formats here. If you are having trouble reading in a specific file format that I have not described, I recommend searching the `pandas` documents for help. \n",
    "\n",
    "## Basic Data Set Info\n",
    "Now that our data set is loaded, let's take a look at some basic features. Once our `pandas` `DataFrame` is loaded, it gains a few functions. \n",
    "\n",
    "The first is `info()` which provides the observation count and variable types. `info()` always prints to the console, so it does not need to be wrapped in a print statement.\n",
    "\n",
    "Next is `head(5)` which will print out the first five observations. We can change the number of observations printed by changing the number\n",
    "\n",
    "Next is `tail(3)` which will print out the last three observations. Similarly, we can change the number to change how many observations are printed\n",
    "\n",
    "Last is `describe()` which provides summary stats for each column in the data frame with numeric data. This is a great way to get a general idea of distributions of variables in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 547 entries, 0 to 546\n",
      "Data columns (total 8 columns):\n",
      "id      547 non-null float64\n",
      "male    547 non-null float64\n",
      "age0    547 non-null float64\n",
      "cd40    547 non-null float64\n",
      "dvl0    547 non-null float64\n",
      "art     547 non-null float64\n",
      "dead    517 non-null float64\n",
      "t       547 non-null float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 34.3 KB\n",
      "First observations:\n",
      "     id  male  age0   cd40  dvl0  art  dead       t\n",
      "0  2.0   1.0  51.0  440.0   1.0  0.0   1.0  32.043\n",
      "1  3.0   1.0  21.0  219.0   0.0  0.0   0.0  60.000\n",
      "2  5.0   1.0  18.0  460.0   0.0  0.0   0.0  60.000\n",
      "3  6.0   1.0  34.0  470.0   1.0  0.0   0.0  60.000\n",
      "4  7.0   1.0  29.0  361.0   0.0  0.0   0.0  60.000\n",
      "5  8.0   1.0  45.0    4.0   1.0  0.0   0.0  60.000\n",
      "Last observations:\n",
      "         id  male  age0   cd40  dvl0  art  dead     t\n",
      "542  795.0   0.0  20.0  494.0   1.0  0.0   0.0  60.0\n",
      "543  796.0   1.0  33.0  458.0   1.0  0.0   0.0  60.0\n",
      "544  798.0   1.0  32.0  237.0   1.0  0.0   0.0  60.0\n",
      "545  799.0   1.0  44.0  488.0   1.0  0.0   0.0  60.0\n",
      "546  800.0   0.0  40.0  209.0   0.0  0.0   0.0  60.0\n",
      "Basic Descriptives:\n",
      "                id        male        age0        cd40        dvl0         art  \\\n",
      "count  547.000000  547.000000  547.000000  547.000000  547.000000  547.000000   \n",
      "mean   400.054845    0.815356   38.519196  303.301645    0.890311    0.144424   \n",
      "std    234.087576    0.388363    9.642183  134.033586    0.312788    0.351841   \n",
      "min      2.000000    0.000000   18.000000    1.000000    0.000000    0.000000   \n",
      "25%    188.000000    1.000000   32.000000  220.500000    1.000000    0.000000   \n",
      "50%    402.000000    1.000000   39.000000  326.000000    1.000000    0.000000   \n",
      "75%    605.500000    1.000000   46.000000  415.500000    1.000000    0.000000   \n",
      "max    800.000000    1.000000   63.000000  500.000000    1.000000    1.000000   \n",
      "\n",
      "             dead           t  \n",
      "count  517.000000  547.000000  \n",
      "mean     0.168279   49.934563  \n",
      "std      0.374476   19.521671  \n",
      "min      0.000000    0.055000  \n",
      "25%      0.000000   57.104000  \n",
      "50%      0.000000   60.000000  \n",
      "75%      0.000000   60.000000  \n",
      "max      1.000000   60.000000  \n"
     ]
    }
   ],
   "source": [
    "# Basic meta-data summary\n",
    "df.info()\n",
    "\n",
    "# print the first 6 observations\n",
    "print('First observations:\\n', df.head(6))\n",
    "\n",
    "# print the last 5 observations\n",
    "print('Last observations:\\n', df.tail(5))\n",
    "\n",
    "# print basic descriptives of numeric columns\n",
    "print('Basic Descriptives:\\n', df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access the list of all the columns in your data set by `print(df.columns())`. I will leave this for you to try on your own.\n",
    "\n",
    "## Selecting / Subsetting Data\n",
    "Now that we have the basic set up of our data frame, let's go over how to select columns and rows\n",
    "\n",
    "### Column Selection\n",
    "For various purposes, we might only want to select a single column. To index a column, you use the following code `df['column_label']` where `column_label` refers to your column label. For our example, we only want to look at the first 3 observations of the `art` column. To do that we can use the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: art, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['art'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alternative to selecting columns and that is to use `df.column_label`. Beware of this option. It is less clear, your column name might overlap with a `pandas` function, it cannot handle multiple columns, and it cannot handle spaces (if they exist in your column labels). I would recommend using `df['column_label']`. I mention the other option since you might come across it in the wild (I sometime use it despite my own advice)\n",
    "\n",
    "To select multiple columns, we can instead provide a list of columns. Below is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     art  dead\n",
      "542  0.0   0.0\n",
      "543  0.0   0.0\n",
      "544  0.0   0.0\n",
      "545  0.0   0.0\n",
      "546  0.0   0.0\n",
      "     art  dead\n",
      "545  0.0   0.0\n",
      "546  0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "# Multiple column selection in one-line\n",
    "print(df[['art', 'dead']].tail(5))\n",
    "\n",
    "# Multiple column selection emphasizing the use of a list of column names\n",
    "columns_to_select = ['art', 'dead']\n",
    "print(df[columns_to_select].tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some column selection basics, let's detail three specific `pandas` operations before moving on; `unique()`, `value_counts()`, and `crosstab`. \n",
    "\n",
    "`unique()` returns all the unique values contained in a column. \n",
    "\n",
    "`value_counts()` is an extension of unique that provides the unique values in a column but also their frequency\n",
    "\n",
    "`crosstab()` creates a crosstabulation of two or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [0. 1.]\n",
      "\n",
      "Counts of unique values:\n",
      " 0.0    468\n",
      "1.0     79\n",
      "Name: art, dtype: int64\n",
      "\n",
      "N-by-M table:\n",
      " dead  0.0  1.0\n",
      "art           \n",
      "0.0   363   77\n",
      "1.0    67   10\n"
     ]
    }
   ],
   "source": [
    "# All unique values in a column\n",
    "print('Unique values:', df['art'].unique())\n",
    "\n",
    "# Unique values and their frequency in a column\n",
    "print('\\nCounts of unique values:\\n', df['art'].value_counts())\n",
    "\n",
    "# Crosstabulation of two variables\n",
    "print('\\nN-by-M table:\\n', pd.crosstab(df['art'], df['dead']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the basics of column selection / subsetting from a data set\n",
    "\n",
    "### Row Selection\n",
    "Now let's go over the basics of row selection. Let's start by selection all the rows in the data set where ART is given (`art=1`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  male  age0   cd40  dvl0  art  dead       t\n",
      "15    28.0   0.0  49.0  226.0   1.0  1.0   0.0  60.000\n",
      "16    29.0   0.0  47.0  332.0   1.0  1.0   0.0  60.000\n",
      "19    32.0   1.0  49.0  413.0   1.0  1.0   0.0  60.000\n",
      "25    39.0   1.0  36.0  424.0   1.0  1.0   0.0  60.000\n",
      "29    45.0   1.0  39.0  274.0   1.0  1.0   0.0  60.000\n",
      "35    52.0   1.0  44.0  309.0   0.0  1.0   0.0  60.000\n",
      "36    54.0   1.0  48.0  192.0   1.0  1.0   0.0  60.000\n",
      "46    70.0   1.0  39.0  126.0   1.0  1.0   0.0  60.000\n",
      "82   114.0   1.0  40.0  123.0   1.0  1.0   0.0  60.000\n",
      "86   119.0   1.0  41.0  391.0   1.0  1.0   0.0  60.000\n",
      "93   127.0   1.0  37.0  408.0   1.0  1.0   0.0  60.000\n",
      "98   134.0   0.0  41.0  345.0   1.0  1.0   0.0  60.000\n",
      "111  155.0   1.0  44.0  139.0   1.0  1.0   0.0  60.000\n",
      "113  158.0   1.0  29.0  216.0   1.0  1.0   0.0  60.000\n",
      "130  180.0   1.0  62.0  204.0   1.0  1.0   0.0  60.000\n",
      "136  187.0   1.0  40.0   90.0   1.0  1.0   1.0   0.094\n",
      "137  189.0   1.0  54.0  301.0   1.0  1.0   0.0  60.000\n",
      "141  197.0   1.0  19.0  224.0   1.0  1.0   0.0  60.000\n",
      "143  201.0   1.0  50.0  381.0   0.0  1.0   0.0  60.000\n",
      "148  211.0   1.0  43.0   47.0   1.0  1.0   0.0  60.000\n",
      "150  214.0   1.0  32.0  215.0   0.0  1.0   1.0   0.974\n",
      "158  228.0   1.0  50.0   83.0   1.0  1.0   1.0   4.491\n",
      "160  231.0   1.0  40.0  128.0   1.0  1.0   0.0  60.000\n",
      "161  232.0   1.0  52.0  462.0   1.0  1.0   0.0  60.000\n",
      "167  240.0   1.0  47.0  352.0   1.0  1.0   NaN  12.734\n",
      "170  243.0   1.0  38.0  192.0   1.0  1.0   0.0  60.000\n",
      "175  249.0   0.0  51.0  421.0   1.0  1.0   0.0  60.000\n",
      "178  253.0   1.0  36.0  278.0   1.0  1.0   0.0  60.000\n",
      "187  269.0   0.0  45.0  417.0   1.0  1.0   1.0   1.389\n",
      "190  275.0   1.0  18.0  352.0   1.0  1.0   0.0  60.000\n",
      "..     ...   ...   ...    ...   ...  ...   ...     ...\n",
      "299  442.0   1.0  44.0  213.0   1.0  1.0   0.0  60.000\n",
      "317  471.0   1.0  35.0  156.0   0.0  1.0   0.0  51.769\n",
      "328  487.0   1.0  34.0   20.0   0.0  1.0   1.0   1.977\n",
      "339  504.0   1.0  48.0  236.0   0.0  1.0   0.0  60.000\n",
      "354  522.0   1.0  40.0  458.0   1.0  1.0   0.0  60.000\n",
      "355  524.0   1.0  32.0  152.0   1.0  1.0   0.0  60.000\n",
      "366  542.0   1.0  32.0  229.0   0.0  1.0   0.0  46.851\n",
      "377  562.0   1.0  42.0  461.0   1.0  1.0   0.0  60.000\n",
      "399  592.0   1.0  23.0  480.0   1.0  1.0   0.0  60.000\n",
      "404  598.0   0.0  35.0  430.0   1.0  1.0   0.0  60.000\n",
      "406  600.0   1.0  45.0  387.0   1.0  1.0   0.0  60.000\n",
      "418  618.0   1.0  40.0  309.0   1.0  1.0   0.0  60.000\n",
      "429  633.0   0.0  29.0  308.0   1.0  1.0   0.0  60.000\n",
      "443  650.0   0.0  29.0  115.0   1.0  1.0   0.0  60.000\n",
      "449  658.0   0.0  23.0  441.0   1.0  1.0   0.0  60.000\n",
      "454  666.0   1.0  48.0  123.0   0.0  1.0   0.0  60.000\n",
      "464  680.0   0.0  35.0  211.0   0.0  1.0   0.0  60.000\n",
      "468  685.0   0.0  36.0  237.0   1.0  1.0   0.0  60.000\n",
      "469  687.0   0.0  53.0  273.0   1.0  1.0   0.0  60.000\n",
      "474  695.0   1.0  29.0    2.0   1.0  1.0   1.0   2.646\n",
      "481  707.0   1.0  25.0  483.0   1.0  1.0   0.0  60.000\n",
      "483  709.0   1.0  43.0   37.0   1.0  1.0   1.0   4.490\n",
      "490  719.0   1.0  41.0  119.0   1.0  1.0   0.0  60.000\n",
      "498  730.0   1.0  18.0  248.0   1.0  1.0   1.0  10.654\n",
      "499  732.0   0.0  22.0  465.0   1.0  1.0   0.0  60.000\n",
      "524  768.0   1.0  31.0  321.0   1.0  1.0   0.0  60.000\n",
      "527  771.0   1.0  45.0   35.0   1.0  1.0   0.0  60.000\n",
      "529  774.0   1.0  59.0  460.0   1.0  1.0   0.0  60.000\n",
      "534  780.0   1.0  51.0  264.0   1.0  1.0   1.0   1.827\n",
      "541  792.0   1.0  25.0  282.0   1.0  1.0   0.0  60.000\n",
      "\n",
      "[79 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df['art']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how did that work? There are two moving parts. First is the part inside the `.loc[...]` function. This section checks whether the condition is met for each row in that column. Pulling out that part results in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "5      False\n",
       "6      False\n",
       "7      False\n",
       "8      False\n",
       "9      False\n",
       "10     False\n",
       "11     False\n",
       "12     False\n",
       "13     False\n",
       "14     False\n",
       "15      True\n",
       "16      True\n",
       "17     False\n",
       "18     False\n",
       "19      True\n",
       "20     False\n",
       "21     False\n",
       "22     False\n",
       "23     False\n",
       "24     False\n",
       "25      True\n",
       "26     False\n",
       "27     False\n",
       "28     False\n",
       "29      True\n",
       "       ...  \n",
       "517    False\n",
       "518    False\n",
       "519    False\n",
       "520    False\n",
       "521    False\n",
       "522    False\n",
       "523    False\n",
       "524     True\n",
       "525    False\n",
       "526    False\n",
       "527     True\n",
       "528    False\n",
       "529     True\n",
       "530    False\n",
       "531    False\n",
       "532    False\n",
       "533    False\n",
       "534     True\n",
       "535    False\n",
       "536    False\n",
       "537    False\n",
       "538    False\n",
       "539    False\n",
       "540    False\n",
       "541     True\n",
       "542    False\n",
       "543    False\n",
       "544    False\n",
       "545    False\n",
       "546    False\n",
       "Name: art, Length: 547, dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['art']==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series of `True` and `False` where the conditions are met or not met, respectively. Then our `.loc` function takes this series of True and False and subsets only the rows where True occurs. \n",
    "\n",
    "Let's take a look at selecting columns based on two conditions. We will select only those who received ART and are above age 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  male  age0   cd40  dvl0  art  dead       t\n",
      "15    28.0   0.0  49.0  226.0   1.0  1.0   0.0  60.000\n",
      "19    32.0   1.0  49.0  413.0   1.0  1.0   0.0  60.000\n",
      "130  180.0   1.0  62.0  204.0   1.0  1.0   0.0  60.000\n",
      "137  189.0   1.0  54.0  301.0   1.0  1.0   0.0  60.000\n",
      "143  201.0   1.0  50.0  381.0   0.0  1.0   0.0  60.000\n",
      "158  228.0   1.0  50.0   83.0   1.0  1.0   1.0   4.491\n",
      "161  232.0   1.0  52.0  462.0   1.0  1.0   0.0  60.000\n",
      "175  249.0   0.0  51.0  421.0   1.0  1.0   0.0  60.000\n",
      "191  276.0   1.0  57.0  326.0   1.0  1.0   0.0  60.000\n",
      "199  284.0   1.0  49.0  131.0   1.0  1.0   0.0  60.000\n",
      "238  346.0   1.0  51.0  338.0   1.0  1.0   0.0  60.000\n",
      "241  353.0   1.0  51.0   68.0   1.0  1.0   0.0  60.000\n",
      "469  687.0   0.0  53.0  273.0   1.0  1.0   0.0  60.000\n",
      "529  774.0   1.0  59.0  460.0   1.0  1.0   0.0  60.000\n",
      "534  780.0   1.0  51.0  264.0   1.0  1.0   1.0   1.827\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[(df['art'] == 1) & (df['age0'] > 48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the inside of the `.loc[...]` follows the standard order of operations. To link multiple conditions together `&` means \"and\", `|` means \"inclusive or\", and `^` is the \"exclusive or\". As a reminder the inclusive or requires that at least one is True, while exclusive or requires that only one is True. \n",
    "\n",
    "Finally, note the parenthenses. If each individual evaluation is not wrapped in a parenthesis, then the conditional will not evaluate. Without the parentheses you will get a `ValueError`\n",
    "\n",
    "#### Side Note:\n",
    "While you may be tempted to use `and` and `or` instead, these operators don't deal with NumPy and Pandas too well. It is better to use `&` and `|`.\n",
    "\n",
    "### Column and Row Selection\n",
    "We now can merge our column and row selection together. To motivate our problem, we are interested in looking at the CD4 T-cell counts by treatment status. Let's look at how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15     226.0\n",
      "16     332.0\n",
      "19     413.0\n",
      "25     424.0\n",
      "29     274.0\n",
      "35     309.0\n",
      "36     192.0\n",
      "46     126.0\n",
      "82     123.0\n",
      "86     391.0\n",
      "93     408.0\n",
      "98     345.0\n",
      "111    139.0\n",
      "113    216.0\n",
      "130    204.0\n",
      "136     90.0\n",
      "137    301.0\n",
      "141    224.0\n",
      "143    381.0\n",
      "148     47.0\n",
      "150    215.0\n",
      "158     83.0\n",
      "160    128.0\n",
      "161    462.0\n",
      "167    352.0\n",
      "170    192.0\n",
      "175    421.0\n",
      "178    278.0\n",
      "187    417.0\n",
      "190    352.0\n",
      "       ...  \n",
      "299    213.0\n",
      "317    156.0\n",
      "328     20.0\n",
      "339    236.0\n",
      "354    458.0\n",
      "355    152.0\n",
      "366    229.0\n",
      "377    461.0\n",
      "399    480.0\n",
      "404    430.0\n",
      "406    387.0\n",
      "418    309.0\n",
      "429    308.0\n",
      "443    115.0\n",
      "449    441.0\n",
      "454    123.0\n",
      "464    211.0\n",
      "468    237.0\n",
      "469    273.0\n",
      "474      2.0\n",
      "481    483.0\n",
      "483     37.0\n",
      "490    119.0\n",
      "498    248.0\n",
      "499    465.0\n",
      "524    321.0\n",
      "527     35.0\n",
      "529    460.0\n",
      "534    264.0\n",
      "541    282.0\n",
      "Name: cd40, Length: 79, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df['art'] == 1, 'cd40'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this example shows, you can use `.loc` to access both selected rows and columns, where the conditions are set up as follows `df.loc[row, column]`. \n",
    "\n",
    "As a demonstration, let's calculate the mean baseline CD4 T-cell count among those given ART with what we have learned and the NumPy function `mean()`. When I subset the data in this example, I will use the `copy()` function at the end. This will return a copy of the subset data. This is good practice to prevent accidentally overwritting something in your original data. Pandas will also generate a warning if you do not create a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247.13924050632912\n",
      "247.13924050632912\n"
     ]
    }
   ],
   "source": [
    "# Subsetting rows and the column of interest\n",
    "dfs = df.loc[df['art'] == 1, 'cd40'].copy()\n",
    "\n",
    "# calculating mean with numpy\n",
    "print(np.mean(dfs))\n",
    "\n",
    "# calculating mean with pandas\n",
    "print(dfs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, I calculated the mean using two different approaches. One used `np.mean()` and the other used `pd.mean()`. These provide the same results (as would be expected). Often there are multiple ways to do things in Python. This is one example.\n",
    "\n",
    "Test yourself by trying to calculate the mean CD4 T-cell count among those not given ART.\n",
    "\n",
    "## Data Operations\n",
    "Now that we have the basics of row and column selection, it is time to introduce some data manipulation. Let's start changing the values of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is a copy of another variable\n",
    "df['art_copy'] = df['art']\n",
    "\n",
    "# Adding constant to a column\n",
    "df['cd4_plus5'] = df['cd40'] + 5\n",
    "\n",
    "# Dividing by constant from a column\n",
    "df['cd4_div12'] = df['cd40'] / 12\n",
    "\n",
    "# Squaring a column\n",
    "df['age_sq'] = df['age0']**2\n",
    "\n",
    "# Using numpy to get square root of column\n",
    "df['age_sqrt'] = np.sqrt(df['age0'])\n",
    "\n",
    "# Adding two columns together\n",
    "df['age_plus_cd4'] = df['age0'] + df['cd40']\n",
    "\n",
    "# Multiple operations\n",
    "df['weird_age'] = (df['age0']*0.5)**2 + (df['cd40']/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few basic examples of operations. Note that these are applied to each row. What if we want to only perform operations on select rows?\n",
    "\n",
    "### Recoding\n",
    "For the purposes of the tutorial, let's say we were concerned about outliers in age. To remove the outliers, we wanted to set everyone older than 50 to 50. How can we do that? I will go through two different approaches\n",
    "\n",
    "#### np.where\n",
    "NumPy `where` is a special function that evaluates a conditional statement for us. Additionally we can use it to generate new numbers. In `np.where(a, b, c)`, `a` is the condition we want to evaluate, `b` is the value given when the condition is True, and `c` is the value when the condition is False.\n",
    "\n",
    "Going to our example, we can to find all those older than 50 (`df['age0'] > 50`) and set them equal to 50 (`50`). Otherwise, we want that row to retain its observed value (`df['age0']`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "# using np.where\n",
    "df['age2_numpy'] = np.where(df['age0'] > 50,  # condition\n",
    "                           50,  # when condition is True\n",
    "                           df['age0'])  # when condition if False\n",
    "\n",
    "print(np.max(df['age2_numpy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the new maximum age is 50, just as our recoding wants. \n",
    "\n",
    "#### df.loc\n",
    "An alternative is to use our friend `loc[...]` to help us out. Using the column selector, we can create a new column and update values. Below is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "df.loc[df['age0'] > 50, 'age2_pandas'] = 50\n",
    "df.loc[df['age0'] <= 50, 'age2_pandas'] = df['age0']\n",
    "\n",
    "print(np.max(df['age2_pandas']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two approaches give the same results. Personally, I like `np.where` but use whatever you prefer. \n",
    "\n",
    "For practice, try making a new age variable where age is squared when between 25 to 45. Otherwise it should be equal to age. You will need what you learned in this section and the row/column selection section to do this. If you have trouble, try going back through the tutorials or open an Issue on my GitHub page\n",
    "\n",
    "## Saving Data\n",
    "Now that you have done all that work, let's save the data. Python does not automatically save your data. You will need to save it as a new file (or overwrite your old one. do **not** recommend this though). To do that, we can use the handy pandas function `to_csv`. This function will have our DataFrame as a CSV. \n",
    "\n",
    "I set `index=False` to keep pandas from saving the index data to the CSV. I don't usually need it, but if you want to save the index, remove this optional argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('saved_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside\n",
    "Before concluding the tutorial, I wanted to return to missing data one more time. Now I will touch on a few unexpected behaviors that `np.nan` has\n",
    "\n",
    "`pd.crosstab` has what I think is an unusual behavior. If you look at the online documentation, you will see that it has the optional argument `dropna`. \"Awesome!\" you think, \"I will be able to easily see the missing data patterns in my data set\". Let's return to our previous crosstab usage, but set `dropna=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead  0.0  1.0\n",
      "art           \n",
      "0.0   363   77\n",
      "1.0    67   10\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(df['art'], df['dead'], dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's weird... there is no missing data column. That's is because the `dropna` argument does not do what you are imagining it does. To see any missing data patterns, we can use `crosstab` but we have to pair it with something. I will use ``fillna()`` to fill in all the missing data points with `-99999`. When using this function to look at missing data patterns, make sure to a value that is not in your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead  -99999.0   0.0       1.0    \n",
      "art                               \n",
      "0.0         28       363        77\n",
      "1.0          2        67        10\n"
     ]
    }
   ],
   "source": [
    "df.fillna(-99999, inplace=True)\n",
    "print(pd.crosstab(df['art'], df['dead']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the missing data pattern! It is generally easier to create a new dataframe with the `nan` filled in rather than using the `inplace` argument as I did. `inplace` means that I would need to use one of the data manipulation approaches to set `-99999` cells back to `nan`. However, this concludes our tutorial\n",
    "\n",
    "# Conclusion\n",
    "That concludes the basic tutorial of data manipulations with NumPy and Pandas. There is a lot I did not cover (handling datetimes, converting between long and wide data sets, mergining data sets, etc.). I will leave those for you to explore. The online documentation for Pandas is pretty great and StackOverflow has the answer to just about any Pandas question you could have. \n",
    "\n",
    "To really learn Pandas, I would recommend replicating some data cleaning you have previously done. I taught myself Pandas by replicating class assignments that I had original done in SAS. This approach allows you to check your answer with a correct one and it teaches you the overlap between functionalities in different softwares. It was about 3 months of doing data manipulation in Python (specifically with NumPy and pandas) that I became adept at it. Practice makes perfect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
